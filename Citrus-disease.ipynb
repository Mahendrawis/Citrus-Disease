import tensorflow as tf
from tensorflow.keras import models, layers
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

BATCH_SIZE = 16
IMAGE_SIZE = 256
EPOCHS = 50

# Path to the dataset
dataset_path = '/content/drive/MyDrive/DatasetCitrus2/Train'

# Load the dataset from Google Drive with a validation split
dataset = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_path,
    shuffle=True,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    validation_split=0.2,
    subset="training",
    seed=42
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_path,
    shuffle=True,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    validation_split=0.2,
    subset="validation",
    seed=42
)

# Preprocess the datasets with data augmentation
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),
    tf.keras.layers.experimental.preprocessing.RandomFlip("horizontal"),
])

train_ds = dataset.map(lambda x, y: (data_augmentation(x, training=True), y)).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.map(lambda x, y: (data_augmentation(x, training=False), y)).prefetch(buffer_size=tf.data.AUTOTUNE)

# Create a CNN model with dropout
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dropout(0.5),
    layers.Dense(128, activation='relu'),
    layers.Dense(2, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Implement early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model with history
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=[early_stopping]
)

# Plot training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Evaluate the model on the test dataset
test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    '/content/drive/MyDrive/DatasetCitrus2/Test',
    shuffle=False,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE
)

# Apply the same data augmentation as in the training and validation datasets
test_ds = test_ds.map(lambda x, y: (data_augmentation(x, training=False), y)).prefetch(buffer_size=tf.data.AUTOTUNE)

test_loss, test_accuracy = model.evaluate(test_ds)
print(f'Test Accuracy: {test_accuracy * 100:.2f}%')

# Confusion Matrix for the model
y_true = np.concatenate([y for x, y in test_ds], axis=0)
y_pred = np.argmax(model.predict(test_ds), axis=1)
cm = confusion_matrix(y_true, y_pred)

# Display Confusion Matrix
class_names = {
    0: "Citrus Canker",
    1: "Black Spot"
}
plt.figure(figsize=(8, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Display all images from the test dataset with predictions and class names
def display_all_images_with_predictions_and_class_names(dataset, num_display=10):
    num_images = min(len(dataset) * BATCH_SIZE, num_display)
    num_cols = 5
    num_rows = (num_images // num_cols) + int(num_images % num_cols > 0)

    plt.figure(figsize=(15, 5 * num_rows))

    accuracies = []

    for i, (image_batch, labels_batch) in enumerate(dataset.take(num_rows)):
        for j in range(image_batch.shape[0]):
            index = i * image_batch.shape[0] + j + 1
            if index > num_images:
                break

            plt.subplot(num_rows, num_cols, index)
            plt.imshow(image_batch[j].numpy().astype("uint8"))

            # Get the predicted class
            predictions_batch = model.predict(np.expand_dims(image_batch[j], axis=0))
            predicted_label_scalar = np.argmax(predictions_batch)
            predicted_class_name = class_names.get(predicted_label_scalar, "Unknown")

            # Get the true class
            true_label_scalar = int(labels_batch[j].numpy())
            true_class_name = class_names.get(true_label_scalar, "Unknown")

            # Calculate accuracy
            accuracy = 100 if predicted_label_scalar == true_label_scalar else 0
            accuracies.append(accuracy)

            # Add title with predicted class name and accuracy
            title = f'Predicted: {predicted_class_name}'
            plt.title(title, fontsize=8)
            plt.axis("off")

            # Print predicted probabilities
            print(f"Sample {index} - Predicted Probabilities: {predictions_batch}")

    # Print accuracy for each image
    for i, accuracy in enumerate(accuracies):
        print(f"Sample {i + 1} - Accuracy: {accuracy:.2f}%")

    plt.show()

# Display all images from the test dataset with predictions and class names
display_all_images_with_predictions_and_class_names(test_ds, num_display=10)

# Print predicted probabilities and accuracy for the first few samples
for i in range(min(5, len(predictions))):
    print(f"Sample {i + 1} - Predicted Probabilities: {predictions[i]}")

# Calculate precision, recall, F1-score, and support
precision, recall, f1_score, support = precision_recall_fscore_support(y_true, y_pred)

# Display precision, recall, F1-score, and support for each class
for i, class_name in class_names.items():
    print(f"Class: {class_name}")
    print(f"Precision: {precision[i]:.2f}")
    print(f"Recall: {recall[i]:.2f}")
    print(f"F1-score: {f1_score[i]:.2f}")
    print(f"Support: {support[i]}")
    print()
